{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c76db644-92c1-4921-813b-189e053b4afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "import kmapper as km\n",
    "import pygeodesic\n",
    "import pygeodesic.geodesic as geodesic\n",
    "from scipy.spatial import distance_matrix\n",
    "import vtk\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa\n",
    "import scipy as sp\n",
    "import networkx as nx\n",
    "import random\n",
    "import math\n",
    "from networkx.algorithms import approximation\n",
    "from scipy.spatial.distance import cdist\n",
    "import open3d as o3d\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12330234-0f2b-446d-a5ea-171f3520ae76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_off_file(filename):\n",
    "    with open(filename) as file:\n",
    "        lines = file.readlines()\n",
    "        lines = [line.strip() for line in lines]\n",
    "        \n",
    "        if lines[0] != \"OFF\":\n",
    "            raise ValueError('Not a valid OFF file')\n",
    "        n_vertices, n_faces, _ = map(int, lines[1].split())\n",
    "        vertices = [tuple(map(float, line.split())) for line in lines[2:2 + n_vertices]]\n",
    "        faces = [tuple(map(int, line.split()[1:])) for line in lines[2 + n_vertices: 2 + n_vertices + n_faces]]\n",
    "\n",
    "        # Create an array of vertex indices\n",
    "        vertex_indices = np.arange(n_vertices)\n",
    "        \n",
    "    return np.array(vertices), np.array(faces), vertex_indices\n",
    "\n",
    "vertices_1, faces_1, vertex_indices_1 = read_off_file('octopus1.off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2020bf20-e378-4bd8-beff-1f754da55ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper_subsample(vertices, D, n_add = 10):\n",
    "\n",
    "    data = vertices\n",
    "    mapper = km.KeplerMapper(verbose=0)\n",
    "    \n",
    "    lens = mapper.fit_transform(data)\n",
    "    \n",
    "    graph = mapper.map(\n",
    "        lens,\n",
    "        data,\n",
    "        clusterer=sklearn.cluster.DBSCAN(eps=0.2, min_samples=3),\n",
    "        cover=km.Cover(n_cubes=60, perc_overlap=0.01),\n",
    "    )\n",
    "\n",
    "    nodes = list(graph['nodes'].keys())\n",
    "    \n",
    "    indexes = []\n",
    "    for i in range(len(nodes)):\n",
    "        tmp_idx = np.array(graph['nodes'][nodes[i]], int)\n",
    "        tmp_d = D[tmp_idx,:][:, tmp_idx].sum(axis=1)\n",
    "        indexes.append(tmp_idx[np.argmin(tmp_d)])\n",
    "\n",
    "    for i in range(n_add):\n",
    "        tmp_D = np.min(D[:,indexes], axis=1)\n",
    "        indexes.append(np.argmax(tmp_D))\n",
    "\n",
    "    print(\"%d points selected by Mapper with %d addition points\" %(len(nodes), n_add))\n",
    "\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dfeb6bf-6bd0-4a9a-a2b6-6671b570a3fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "geoalg = geodesic.PyGeodesicAlgorithmExact(vertices_1, faces_1)\n",
    "D_1 = np.zeros((len(vertex_indices_1), len(vertex_indices_1)))\n",
    "for i in range(D_1.shape[0]):\n",
    "    source_indices = np.array([vertex_indices_1[i]]) \n",
    "    target_indices = np.array(vertex_indices_1)\n",
    "    distancess, best_source = geoalg.geodesicDistances(source_indices, target_indices)\n",
    "    D_1[i] = distancess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d05b693d-50f6-49c3-9732-b71612a1b582",
   "metadata": {},
   "outputs": [],
   "source": [
    "#half leg1 that need to be cut off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d161a535-b019-4d9f-a9bc-2f772f222b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,  45,  46,  47,  48,  49,  50,  52,  53,  55,  57, 123,\n",
       "       124, 126, 128, 131, 196, 203, 206, 207, 209, 211, 214, 216, 220,\n",
       "       294, 299, 301, 302, 306, 307, 309, 312, 314, 319, 407, 411, 413,\n",
       "       414, 422, 426, 427, 432, 548, 549, 556, 560, 563, 564, 565, 567,\n",
       "       568, 572, 575])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geoalg = geodesic.PyGeodesicAlgorithmExact(vertices_1, faces_1)\n",
    "\n",
    "test, best = geoalg.geodesicDistances([550-1], [575-1])\n",
    "\n",
    "source_indices = np.array([550-1]) \n",
    "target_indices = np.array(vertex_indices_1)\n",
    "distancess, best_source = geoalg.geodesicDistances(source_indices, target_indices)\n",
    "\n",
    "target_indices = np.array(vertex_indices_1)\n",
    "\n",
    "halfl_indices_less_than_test_1 = target_indices[distancess < test]\n",
    "halfl_leg1 = halfl_indices_less_than_test_1.tolist()\n",
    "halfl_indices_less_than_test_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19622a6d-bad9-41be-9af6-a83f09273d57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#half leg2 that need to be cut off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b81e2159-95a6-4741-97ae-39b6f7edc668",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 69,  71,  72,  75,  76,  77,  78,  82, 146, 147, 148, 228, 231,\n",
       "       232, 239, 240, 335, 336, 339, 340, 341, 343, 345, 451, 454, 455,\n",
       "       602, 603, 604, 605, 610, 611])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geoalg = geodesic.PyGeodesicAlgorithmExact(vertices_1, faces_1)\n",
    "\n",
    "test, best = geoalg.geodesicDistances([456-1], [600-1])\n",
    "\n",
    "source_indices = np.array([456-1]) \n",
    "target_indices = np.array(vertex_indices_1)\n",
    "distancess, best_source = geoalg.geodesicDistances(source_indices, target_indices)\n",
    "\n",
    "target_indices = np.array(vertex_indices_1)\n",
    "\n",
    "halfl_indices_less_than_test_2 = target_indices[distancess < test]\n",
    "halfl_leg2 = halfl_indices_less_than_test_2.tolist()\n",
    "halfl_indices_less_than_test_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c86cc49a-33f4-4c06-9ead-3df26b9f3977",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#half leg3 that need to be cut off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a8a45cf-988e-4d82-9e6e-7f0de60fe034",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 81,  86,  87,  89,  92,  93,  94,  96,  98, 149, 151, 153, 241,\n",
       "       243, 245, 246, 247, 248, 348, 349, 352, 353, 354, 356, 358, 457,\n",
       "       460, 461, 462, 463, 464, 465, 466, 467, 470, 615, 616, 620, 621,\n",
       "       623, 625, 627, 628])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geoalg = geodesic.PyGeodesicAlgorithmExact(vertices_1, faces_1)\n",
    "\n",
    "test, best = geoalg.geodesicDistances([248-1], [610-1])\n",
    "\n",
    "source_indices = np.array([248-1]) \n",
    "target_indices = np.array(vertex_indices_1)\n",
    "distancess, best_source = geoalg.geodesicDistances(source_indices, target_indices)\n",
    "\n",
    "target_indices = np.array(vertex_indices_1)\n",
    "\n",
    "halfl_indices_less_than_test_3 = target_indices[distancess < test]\n",
    "halfl_leg3 = halfl_indices_less_than_test_3.tolist()\n",
    "halfl_indices_less_than_test_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21dfd91b-7d92-41f3-95ee-02f512c5c716",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#half leg4 that need to be cut off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d780b02e-71e4-420d-bbb8-0f5f318a151e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 84,  85,  88,  90,  91,  95,  97,  99, 100, 101, 102, 145, 150,\n",
       "       152, 154, 236, 242, 244, 249, 346, 350, 351, 355, 357, 359, 360,\n",
       "       361, 362, 456, 458, 459, 468, 469, 607, 612, 613, 614, 617, 618,\n",
       "       619, 622, 624, 626, 629, 630, 631, 632])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geoalg = geodesic.PyGeodesicAlgorithmExact(vertices_1, faces_1)\n",
    "\n",
    "test, best = geoalg.geodesicDistances([250-1], [607-1])\n",
    "\n",
    "source_indices = np.array([250-1]) \n",
    "target_indices = np.array(vertex_indices_1)\n",
    "distancess, best_source = geoalg.geodesicDistances(source_indices, target_indices)\n",
    "\n",
    "target_indices = np.array(vertex_indices_1)\n",
    "\n",
    "halfl_indices_less_than_test_4 = target_indices[distancess < test]\n",
    "halfl_leg4 = halfl_indices_less_than_test_4.tolist()\n",
    "halfl_indices_less_than_test_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d654719f-665b-47d0-a036-26cfbd37cf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#half leg5 that need to be cut off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f92024e8-d85f-4452-a633-67e0ad4ac13c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([197, 200, 201, 204, 205, 208, 212, 217, 297, 298, 300, 303, 304,\n",
       "       308, 313, 412, 415, 421, 423, 433, 525, 531, 532, 542, 550, 554,\n",
       "       555, 561, 647, 655, 662, 663, 665, 668, 677, 682, 691, 718, 719,\n",
       "       726, 727, 733, 734, 740, 743, 747, 750, 754, 758, 766, 767])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geoalg = geodesic.PyGeodesicAlgorithmExact(vertices_1, faces_1)\n",
    "\n",
    "test, best = geoalg.geodesicDistances([434-1], [736-1])\n",
    "\n",
    "source_indices = np.array([434-1]) \n",
    "target_indices = np.array(vertex_indices_1)\n",
    "distancess, best_source = geoalg.geodesicDistances(source_indices, target_indices)\n",
    "\n",
    "target_indices = np.array(vertex_indices_1)\n",
    "\n",
    "halfl_indices_less_than_test_5 = target_indices[distancess < test]\n",
    "halfl_leg5 = halfl_indices_less_than_test_5.tolist()\n",
    "halfl_indices_less_than_test_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7457797c-77cb-4b07-8669-99f7b796d380",
   "metadata": {},
   "outputs": [],
   "source": [
    "#half leg6 that need to be cut off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebf5bcc3-c399-408f-a4b6-08bc1ffbd222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 22,  23,  24,  26,  27,  30,  31,  32,  33, 108, 110, 111, 112,\n",
       "       113, 118, 170, 171, 172, 173, 178, 179, 180, 181, 182, 262, 266,\n",
       "       273, 275, 276, 278, 377, 378, 381, 383, 388, 389, 390, 391, 491,\n",
       "       492, 493, 494, 498, 499, 501, 502, 503, 508])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geoalg = geodesic.PyGeodesicAlgorithmExact(vertices_1, faces_1)\n",
    "\n",
    "test, best = geoalg.geodesicDistances([276-1], [505-1])\n",
    "\n",
    "source_indices = np.array([276-1]) \n",
    "target_indices = np.array(vertex_indices_1)\n",
    "distancess, best_source = geoalg.geodesicDistances(source_indices, target_indices)\n",
    "\n",
    "target_indices = np.array(vertex_indices_1)\n",
    "\n",
    "halfl_indices_less_than_test_6 = target_indices[distancess < test]\n",
    "halfl_leg6 = halfl_indices_less_than_test_6.tolist()\n",
    "halfl_indices_less_than_test_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60520680-4da4-4986-9944-860dfb1f4035",
   "metadata": {},
   "outputs": [],
   "source": [
    "#half leg7 that need to be cut off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "655f7c4a-1270-4aef-afe3-627e0fba3385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5,   8,   9,  10,  14,  15, 103, 105, 106, 156, 157, 158, 161,\n",
       "       162, 163, 165, 251, 252, 253, 255, 257, 260, 263, 365, 366, 367,\n",
       "       370, 372, 475, 476, 479, 480, 481, 483, 484])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geoalg = geodesic.PyGeodesicAlgorithmExact(vertices_1, faces_1)\n",
    "\n",
    "test, best = geoalg.geodesicDistances([104-1], [486-1])\n",
    "\n",
    "source_indices = np.array([104-1]) \n",
    "target_indices = np.array(vertex_indices_1)\n",
    "distancess, best_source = geoalg.geodesicDistances(source_indices, target_indices)\n",
    "\n",
    "target_indices = np.array(vertex_indices_1)\n",
    "\n",
    "halfl_indices_less_than_test_7 = target_indices[distancess < test]\n",
    "halfl_leg7 = halfl_indices_less_than_test_7.tolist()\n",
    "halfl_indices_less_than_test_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ff1bd64-29c2-4f81-99c9-1e55d049b6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#half leg8 that need to be cut off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb02de44-9dae-49f8-8406-210d180c28c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3,   4,   6,   7,  11,  12,  13,  18,  19,  20,  21, 104, 107,\n",
       "       155, 159, 160, 164, 166, 167, 168, 169, 250, 254, 256, 258, 259,\n",
       "       265, 269, 271, 363, 364, 368, 369, 371, 373, 375, 376, 380, 382,\n",
       "       471, 472, 473, 474, 477, 478, 482, 486, 487, 489, 490])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geoalg = geodesic.PyGeodesicAlgorithmExact(vertices_1, faces_1)\n",
    "\n",
    "test, best = geoalg.geodesicDistances([4-1], [501-1])\n",
    "\n",
    "source_indices = np.array([4-1]) \n",
    "target_indices = np.array(vertex_indices_1)\n",
    "distancess, best_source = geoalg.geodesicDistances(source_indices, target_indices)\n",
    "\n",
    "target_indices = np.array(vertex_indices_1)\n",
    "\n",
    "halfl_indices_less_than_test_8 = target_indices[distancess < test]\n",
    "halfl_leg8 = halfl_indices_less_than_test_8.tolist()\n",
    "halfl_indices_less_than_test_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5804f0a7-e0e5-4baa-8683-19dcdc99ac71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#half octopus1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bcce7b69-31be-4436-8437-4705dc98b208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   2,   16,   17,   25,   28,   29,   34,   35,   36,   37,   38,\n",
       "         39,   40,   41,   42,   43,   44,   51,   54,   56,   58,   59,\n",
       "         60,   61,   62,   63,   64,   65,   66,   67,   68,   70,   73,\n",
       "         74,   79,   80,   83,  109,  114,  115,  116,  117,  119,  120,\n",
       "        121,  122,  125,  127,  129,  130,  132,  133,  134,  135,  136,\n",
       "        137,  138,  139,  140,  141,  142,  143,  144,  174,  175,  176,\n",
       "        177,  183,  184,  185,  186,  187,  188,  189,  190,  191,  192,\n",
       "        193,  194,  195,  198,  199,  202,  210,  213,  215,  218,  219,\n",
       "        221,  222,  223,  224,  225,  226,  227,  229,  230,  233,  234,\n",
       "        235,  237,  238,  261,  264,  267,  268,  270,  272,  274,  277,\n",
       "        279,  280,  281,  282,  283,  284,  285,  286,  287,  288,  289,\n",
       "        290,  291,  292,  293,  295,  296,  305,  310,  311,  315,  316,\n",
       "        317,  318,  320,  321,  322,  323,  324,  325,  326,  327,  328,\n",
       "        329,  330,  331,  332,  333,  334,  337,  338,  342,  344,  347,\n",
       "        374,  379,  384,  385,  386,  387,  392,  393,  394,  395,  396,\n",
       "        397,  398,  399,  400,  401,  402,  403,  404,  405,  406,  408,\n",
       "        409,  410,  416,  417,  418,  419,  420,  424,  425,  428,  429,\n",
       "        430,  431,  434,  435,  436,  437,  438,  439,  440,  441,  442,\n",
       "        443,  444,  445,  446,  447,  448,  449,  450,  452,  453,  485,\n",
       "        488,  495,  496,  497,  500,  504,  505,  506,  507,  509,  510,\n",
       "        511,  512,  513,  514,  515,  516,  517,  518,  519,  520,  521,\n",
       "        522,  523,  524,  526,  527,  528,  529,  530,  533,  534,  535,\n",
       "        536,  537,  538,  539,  540,  541,  543,  544,  545,  546,  547,\n",
       "        551,  552,  553,  557,  558,  559,  562,  566,  569,  570,  571,\n",
       "        573,  574,  576,  577,  578,  579,  580,  581,  582,  583,  584,\n",
       "        585,  586,  587,  588,  589,  590,  591,  592,  593,  594,  595,\n",
       "        596,  597,  598,  599,  600,  601,  606,  608,  609,  633,  634,\n",
       "        635,  636,  637,  638,  639,  640,  641,  642,  643,  644,  645,\n",
       "        646,  648,  649,  650,  651,  652,  653,  654,  656,  657,  658,\n",
       "        659,  660,  661,  664,  666,  667,  669,  670,  671,  672,  673,\n",
       "        674,  675,  676,  678,  679,  680,  681,  683,  684,  685,  686,\n",
       "        687,  688,  689,  690,  692,  693,  694,  695,  696,  697,  698,\n",
       "        699,  700,  701,  702,  703,  704,  705,  706,  707,  708,  709,\n",
       "        710,  711,  712,  713,  714,  715,  716,  717,  720,  721,  722,\n",
       "        723,  724,  725,  728,  729,  730,  731,  732,  735,  736,  737,\n",
       "        738,  739,  741,  742,  744,  745,  746,  748,  749,  751,  752,\n",
       "        753,  755,  756,  757,  759,  760,  761,  762,  763,  764,  765,\n",
       "        768,  769,  770,  771,  772,  773,  774,  775,  776,  777,  778,\n",
       "        779,  780,  781,  782,  783,  784,  785,  786,  787,  788,  789,\n",
       "        790,  791,  792,  793,  794,  795,  796,  797,  798,  799,  800,\n",
       "        801,  802,  803,  804,  805,  806,  807,  808,  809,  810,  811,\n",
       "        812,  813,  814,  815,  816,  817,  818,  819,  820,  821,  822,\n",
       "        823,  824,  825,  826,  827,  828,  829,  830,  831,  832,  833,\n",
       "        834,  835,  836,  837,  838,  839,  840,  841,  842,  843,  844,\n",
       "        845,  846,  847,  848,  849,  850,  851,  852,  853,  854,  855,\n",
       "        856,  857,  858,  859,  860,  861,  862,  863,  864,  865,  866,\n",
       "        867,  868,  869,  870,  871,  872,  873,  874,  875,  876,  877,\n",
       "        878,  879,  880,  881,  882,  883,  884,  885,  886,  887,  888,\n",
       "        889,  890,  891,  892,  893,  894,  895,  896,  897,  898,  899,\n",
       "        900,  901,  902,  903,  904,  905,  906,  907,  908,  909,  910,\n",
       "        911,  912,  913,  914,  915,  916,  917,  918,  919,  920,  921,\n",
       "        922,  923,  924,  925,  926,  927,  928,  929,  930,  931,  932,\n",
       "        933,  934,  935,  936,  937,  938,  939,  940,  941,  942,  943,\n",
       "        944,  945,  946,  947,  948,  949,  950,  951,  952,  953,  954,\n",
       "        955,  956,  957,  958,  959,  960,  961,  962,  963,  964,  965,\n",
       "        966,  967,  968,  969,  970,  971,  972,  973,  974,  975,  976,\n",
       "        977,  978,  979,  980,  981,  982,  983,  984,  985,  986,  987,\n",
       "        988,  989,  990,  991,  992,  993,  994,  995,  996,  997,  998,\n",
       "        999, 1000, 1001])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined = halfl_leg1 + halfl_leg2 + halfl_leg3 + halfl_leg4 + halfl_leg5 + halfl_leg6 + halfl_leg7 + halfl_leg8\n",
    "halfl_body = [item for item in vertex_indices_1 if item not in combined]\n",
    "\n",
    "indices_less_than_test_9 = np.array(halfl_body)\n",
    "indices_less_than_test_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7444b6b-7b0c-48fc-bf8a-9283f0063175",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# downsampled half_octopus1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7cd5d013-5b26-41ae-a507-badeb8728c79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134 points selected by Mapper with 10 addition points\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/salovjade/anaconda3/lib/python3.10/site-packages/threadpoolctl.py:1010: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  17,   28,   38,   64,   83,  114,  136,  174,  198,  218,  268,\n",
       "        270,  288,  328,  392,  400,  416,  509,  523,  539,  547,  574,\n",
       "        581,  582,  597,  608,  634,  667,  678,  686,  687,  693,  700,\n",
       "        703,  708,  712,  716,  721,  731,  742,  748,  752,  756,  759,\n",
       "        761,  775,  782,  784,  800,  809,  818,  831,  834,  836,  882,\n",
       "        883,  898,  906,  909,  914,  931,  935,  939,  944,  947,  948,\n",
       "        956,  968,  971,  984,  987,  992,  993,  994,  997,  999, 1001])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = mapper_subsample(vertices_1, D_1, n_add=10)\n",
    "\n",
    "idx_1 = np.intersect1d(idx, halfl_body)\n",
    "\n",
    "highlightIndices1 = np.array(idx_1)\n",
    "highlightIndices1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674551a2-4a04-4059-be12-c15013799e6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "73f9ad36-a32f-46dd-98dd-d853045d6115",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import idx_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd323d1e-e834-491f-b1df-f71fb0bfae4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117 points selected by Mapper with 10 addition points\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([925, 841, 843, 817, 818, 776, 773, 753, 684, 682, 983, 614, 957,\n",
       "       579, 448, 934, 208, 510, 874, 169, 421, 587, 889, 106, 419, 835,\n",
       "       105, 852, 882, 105, 794, 125, 770, 881, 124, 418, 699, 168, 480,\n",
       "       416, 976, 424, 356, 314, 381, 955, 230, 404, 182, 790, 812, 166,\n",
       "       379, 164, 162, 458, 853, 177, 437, 250, 375, 655, 830, 244, 238,\n",
       "       242, 805, 868, 192, 378, 865, 189, 220, 821, 133, 240, 849, 115,\n",
       "       139, 333, 810, 119, 272,  56,  99, 436,  63,  78, 200, 468,  55,\n",
       "        71, 273, 493,  61, 159, 465,  60, 143, 492,  59, 123, 528,  25,\n",
       "        39,   8,  30,   1,   0,  20,   6,  21,  16,  12,  11,   2,   4,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vertices_2, faces_2, vertex_indices_2 = read_off_file('octopus2.off')\n",
    "\n",
    "geoalg = geodesic.PyGeodesicAlgorithmExact(vertices_2, faces_2)\n",
    "D_2 = np.zeros((len(vertex_indices_2), len(vertex_indices_2)))\n",
    "for i in range(D_2.shape[0]):\n",
    "    source_indices = np.array([vertex_indices_2[i]]) \n",
    "    target_indices = np.array(vertex_indices_2)\n",
    "    distancess, best_source = geoalg.geodesicDistances(source_indices, target_indices)\n",
    "    D_1[i] = distancess\n",
    "    \n",
    "idx_2 = mapper_subsample(vertices_2, D_2, n_add=10)  \n",
    "\n",
    "highlightIndices2 = np.array(idx_2)\n",
    "highlightIndices2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09f9f301-17bb-409f-86f7-29eefa3bc437",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign half legs matching from each P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "346d509e-bc6c-483b-b68c-ebcb4e56ba56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
       "        56,  57,  58, 123, 124, 125, 126, 127, 128, 129, 130, 131, 135,\n",
       "       196, 203, 206, 207, 209, 210, 211, 214, 215, 216, 218, 219, 220,\n",
       "       294, 299, 301, 302, 305, 306, 307, 309, 311, 312, 314, 316, 317,\n",
       "       318, 319, 407, 411, 413, 414, 417, 418, 419, 420, 422, 426, 427,\n",
       "       430, 431, 432, 436, 548, 549, 556, 560, 562, 563, 564, 565, 567,\n",
       "       568, 571, 572, 573, 574, 575, 578, 579, 582, 671, 681, 687, 689,\n",
       "       690, 745, 746, 749, 753, 794, 797, 800, 828])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geoalg = geodesic.PyGeodesicAlgorithmExact(vertices_1, faces_1)\n",
    "\n",
    "test, best = geoalg.geodesicDistances([550-1], [832-1])\n",
    "\n",
    "source_indices = np.array([550-1]) \n",
    "target_indices = np.array(vertex_indices_1)\n",
    "distancess, best_source = geoalg.geodesicDistances(source_indices, target_indices)\n",
    "\n",
    "target_indices = np.array(vertex_indices_1)\n",
    "\n",
    "indices_less_than_test_1 = target_indices[distancess < test]\n",
    "leg1 = indices_less_than_test_1.tolist()\n",
    "indices_less_than_test_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4178bc4a-7e84-4f69-a09b-0d7f5b173d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2,  64,  65,  67,  69,  71,  72,  75,  76,  77,  78,  82, 137,\n",
       "       138, 140, 142, 146, 147, 148, 223, 227, 228, 230, 231, 232, 239,\n",
       "       240, 324, 325, 329, 331, 335, 336, 338, 339, 340, 341, 343, 345,\n",
       "       424, 434, 435, 440, 442, 445, 446, 451, 454, 455, 584, 585, 589,\n",
       "       590, 593, 596, 598, 599, 602, 603, 604, 605, 610, 611, 688, 765,\n",
       "       769, 771, 806, 834])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geoalg = geodesic.PyGeodesicAlgorithmExact(vertices_1, faces_1)\n",
    "\n",
    "test, best = geoalg.geodesicDistances([456-1], [841-1])\n",
    "\n",
    "source_indices = np.array([456-1]) \n",
    "target_indices = np.array(vertex_indices_1)\n",
    "distancess, best_source = geoalg.geodesicDistances(source_indices, target_indices)\n",
    "\n",
    "target_indices = np.array(vertex_indices_1)\n",
    "\n",
    "indices_less_than_test_2 = target_indices[distancess < test]\n",
    "leg2 = indices_less_than_test_2.tolist()\n",
    "indices_less_than_test_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "142d731a-71c3-41bc-9066-ea0880050a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 63,  73,  74,  80,  81,  83,  86,  87,  89,  92,  93,  94,  96,\n",
       "        98, 141, 144, 149, 151, 153, 221, 226, 237, 238, 241, 243, 245,\n",
       "       246, 247, 248, 321, 322, 328, 330, 333, 337, 344, 348, 349, 352,\n",
       "       353, 354, 356, 358, 438, 441, 448, 449, 453, 457, 460, 461, 462,\n",
       "       463, 464, 465, 466, 467, 470, 583, 600, 608, 609, 615, 616, 620,\n",
       "       621, 623, 625, 627, 628, 694, 695, 696, 697, 698, 770, 808, 809])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geoalg = geodesic.PyGeodesicAlgorithmExact(vertices_1, faces_1)\n",
    "\n",
    "test, best = geoalg.geodesicDistances([248-1], [842-1])\n",
    "\n",
    "source_indices = np.array([248-1]) \n",
    "target_indices = np.array(vertex_indices_1)\n",
    "distancess, best_source = geoalg.geodesicDistances(source_indices, target_indices)\n",
    "\n",
    "target_indices = np.array(vertex_indices_1)\n",
    "\n",
    "indices_less_than_test_3 = target_indices[distancess < test]\n",
    "leg3 = indices_less_than_test_3.tolist()\n",
    "indices_less_than_test_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "71a19a9b-2c2e-43dc-9b3a-1f31f07f025b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 59,  60,  61,  62,  66,  68,  70,  79,  84,  85,  88,  90,  91,\n",
       "        95,  97,  99, 100, 101, 102, 132, 133, 134, 136, 139, 143, 145,\n",
       "       150, 152, 154, 213, 222, 224, 225, 229, 233, 234, 235, 236, 242,\n",
       "       244, 249, 320, 323, 326, 327, 332, 334, 342, 346, 347, 350, 351,\n",
       "       355, 357, 359, 360, 361, 362, 416, 428, 437, 439, 443, 444, 447,\n",
       "       450, 452, 456, 458, 459, 468, 469, 576, 586, 587, 588, 591, 592,\n",
       "       594, 595, 597, 601, 606, 607, 612, 613, 614, 617, 618, 619, 622,\n",
       "       624, 626, 629, 630, 631, 632, 683, 684, 692, 693, 755, 759, 760,\n",
       "       761, 795, 796])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geoalg = geodesic.PyGeodesicAlgorithmExact(vertices_1, faces_1)\n",
    "\n",
    "test, best = geoalg.geodesicDistances([250-1], [799-1])\n",
    "\n",
    "source_indices = np.array([250-1]) \n",
    "target_indices = np.array(vertex_indices_1)\n",
    "distancess, best_source = geoalg.geodesicDistances(source_indices, target_indices)\n",
    "\n",
    "target_indices = np.array(vertex_indices_1)\n",
    "\n",
    "indices_less_than_test_4 = target_indices[distancess < test]\n",
    "leg4 = indices_less_than_test_4.tolist()\n",
    "indices_less_than_test_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86442594-e1c0-4d94-8ce4-ec0485396e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([194, 195, 197, 198, 199, 200, 201, 202, 204, 205, 208, 212, 217,\n",
       "       287, 288, 293, 295, 296, 297, 298, 300, 303, 304, 308, 313, 402,\n",
       "       408, 409, 410, 412, 415, 421, 423, 433, 522, 525, 526, 527, 531,\n",
       "       532, 542, 544, 550, 551, 552, 554, 555, 561, 642, 643, 647, 655,\n",
       "       656, 657, 658, 660, 662, 663, 665, 668, 677, 682, 691, 703, 706,\n",
       "       707, 712, 718, 719, 720, 721, 723, 724, 726, 727, 728, 729, 730,\n",
       "       733, 734, 735, 740, 743, 747, 750, 754, 758, 766, 767, 780, 787,\n",
       "       789])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geoalg = geodesic.PyGeodesicAlgorithmExact(vertices_1, faces_1)\n",
    "\n",
    "test, best = geoalg.geodesicDistances([434-1], [819-1])\n",
    "\n",
    "source_indices = np.array([434-1]) \n",
    "target_indices = np.array(vertex_indices_1)\n",
    "distancess, best_source = geoalg.geodesicDistances(source_indices, target_indices)\n",
    "\n",
    "target_indices = np.array(vertex_indices_1)\n",
    "\n",
    "indices_less_than_test_5 = target_indices[distancess < test]\n",
    "leg5 = indices_less_than_test_5.tolist()\n",
    "indices_less_than_test_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bca03117-1061-4e92-8fef-b36fa9609ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 22,  23,  24,  26,  27,  28,  30,  31,  32,  33,  37,  38,  39,\n",
       "        41, 108, 110, 111, 112, 113, 114, 118, 119, 121, 170, 171, 172,\n",
       "       173, 176, 178, 179, 180, 181, 182, 186, 188, 189, 262, 266, 267,\n",
       "       273, 275, 276, 277, 278, 279, 284, 377, 378, 381, 383, 384, 388,\n",
       "       389, 390, 391, 393, 396, 404, 491, 492, 493, 494, 495, 498, 499,\n",
       "       501, 502, 503, 504, 508, 509, 510, 512, 521, 523, 528, 634, 635,\n",
       "       636, 644, 646, 699, 702, 772, 778])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geoalg = geodesic.PyGeodesicAlgorithmExact(vertices_1, faces_1)\n",
    "\n",
    "test, best = geoalg.geodesicDistances([276-1], [775-1])\n",
    "\n",
    "source_indices = np.array([276-1]) \n",
    "target_indices = np.array(vertex_indices_1)\n",
    "distancess, best_source = geoalg.geodesicDistances(source_indices, target_indices)\n",
    "\n",
    "target_indices = np.array(vertex_indices_1)\n",
    "\n",
    "indices_less_than_test_6 = target_indices[distancess < test]\n",
    "leg6 = indices_less_than_test_6.tolist()\n",
    "indices_less_than_test_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "91ee9bdc-7222-462c-816d-3565b7674707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5,   8,   9,  10,  14,  15,  16,  17,  25,  29,  36, 103, 105,\n",
       "       106, 109, 115, 117, 156, 157, 158, 161, 162, 163, 165, 174, 175,\n",
       "       177, 183, 187, 251, 252, 253, 255, 257, 260, 261, 263, 264, 268,\n",
       "       270, 280, 281, 283, 365, 366, 367, 370, 372, 374, 379, 385, 386,\n",
       "       399, 405, 475, 476, 479, 480, 481, 483, 484, 485, 488, 496, 497,\n",
       "       505, 506, 514, 515, 518, 524, 537, 633, 639, 700, 701, 711, 773,\n",
       "       775, 776, 783])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geoalg = geodesic.PyGeodesicAlgorithmExact(vertices_1, faces_1)\n",
    "\n",
    "test, best = geoalg.geodesicDistances([104-1], [818-1])\n",
    "\n",
    "source_indices = np.array([104-1]) \n",
    "target_indices = np.array(vertex_indices_1)\n",
    "distancess, best_source = geoalg.geodesicDistances(source_indices, target_indices)\n",
    "\n",
    "target_indices = np.array(vertex_indices_1)\n",
    "\n",
    "indices_less_than_test_7 = target_indices[distancess < test]\n",
    "leg7 = indices_less_than_test_7.tolist()\n",
    "indices_less_than_test_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2314b8b6-0929-4409-a38f-0dc7ca042aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3,   4,   6,   7,  11,  12,  13,  18,  19,  20,  21,  34,  35,\n",
       "        40,  42,  43,  44, 104, 107, 116, 120, 122, 155, 159, 160, 164,\n",
       "       166, 167, 168, 169, 184, 185, 190, 191, 192, 193, 250, 254, 256,\n",
       "       258, 259, 265, 269, 271, 272, 274, 282, 286, 289, 290, 292, 363,\n",
       "       364, 368, 369, 371, 373, 375, 376, 380, 382, 387, 392, 394, 395,\n",
       "       397, 400, 401, 406, 471, 472, 473, 474, 477, 478, 482, 486, 487,\n",
       "       489, 490, 500, 507, 511, 513, 516, 517, 519, 520, 529, 530, 640,\n",
       "       641, 649, 704, 705, 717, 737, 785, 788, 821, 822])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geoalg = geodesic.PyGeodesicAlgorithmExact(vertices_1, faces_1)\n",
    "\n",
    "test, best = geoalg.geodesicDistances([4-1], [845-1])\n",
    "\n",
    "source_indices = np.array([4-1]) \n",
    "target_indices = np.array(vertex_indices_1)\n",
    "distancess, best_source = geoalg.geodesicDistances(source_indices, target_indices)\n",
    "\n",
    "target_indices = np.array(vertex_indices_1)\n",
    "\n",
    "indices_less_than_test_8 = target_indices[distancess < test]\n",
    "leg8 = indices_less_than_test_8.tolist()\n",
    "indices_less_than_test_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "762f32c5-7697-4ec3-a2e1-81a9799fc417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 285,  291,  310,  315,  398,  403,  425,  429,  533,  534,  535,\n",
       "        536,  538,  539,  540,  541,  543,  545,  546,  547,  553,  557,\n",
       "        558,  559,  566,  569,  570,  577,  580,  581,  637,  638,  645,\n",
       "        648,  650,  651,  652,  653,  654,  659,  661,  664,  666,  667,\n",
       "        669,  670,  672,  673,  674,  675,  676,  678,  679,  680,  685,\n",
       "        686,  708,  709,  710,  713,  714,  715,  716,  722,  725,  731,\n",
       "        732,  736,  738,  739,  741,  742,  744,  748,  751,  752,  756,\n",
       "        757,  762,  763,  764,  768,  774,  777,  779,  781,  782,  784,\n",
       "        786,  790,  791,  792,  793,  798,  799,  801,  802,  803,  804,\n",
       "        805,  807,  810,  811,  812,  813,  814,  815,  816,  817,  818,\n",
       "        819,  820,  823,  824,  825,  826,  827,  829,  830,  831,  832,\n",
       "        833,  835,  836,  837,  838,  839,  840,  841,  842,  843,  844,\n",
       "        845,  846,  847,  848,  849,  850,  851,  852,  853,  854,  855,\n",
       "        856,  857,  858,  859,  860,  861,  862,  863,  864,  865,  866,\n",
       "        867,  868,  869,  870,  871,  872,  873,  874,  875,  876,  877,\n",
       "        878,  879,  880,  881,  882,  883,  884,  885,  886,  887,  888,\n",
       "        889,  890,  891,  892,  893,  894,  895,  896,  897,  898,  899,\n",
       "        900,  901,  902,  903,  904,  905,  906,  907,  908,  909,  910,\n",
       "        911,  912,  913,  914,  915,  916,  917,  918,  919,  920,  921,\n",
       "        922,  923,  924,  925,  926,  927,  928,  929,  930,  931,  932,\n",
       "        933,  934,  935,  936,  937,  938,  939,  940,  941,  942,  943,\n",
       "        944,  945,  946,  947,  948,  949,  950,  951,  952,  953,  954,\n",
       "        955,  956,  957,  958,  959,  960,  961,  962,  963,  964,  965,\n",
       "        966,  967,  968,  969,  970,  971,  972,  973,  974,  975,  976,\n",
       "        977,  978,  979,  980,  981,  982,  983,  984,  985,  986,  987,\n",
       "        988,  989,  990,  991,  992,  993,  994,  995,  996,  997,  998,\n",
       "        999, 1000, 1001])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined = leg1 + leg2 + leg3 + leg4 + leg5 + leg6 + leg7 + leg8\n",
    "body = [item for item in vertex_indices_1 if item not in combined]\n",
    "\n",
    "indices_less_than_test_9 = np.array(body)\n",
    "indices_less_than_test_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bc2c15-0df4-436b-a78d-85508700fdac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab8c338-50d3-42a0-bbf9-f1dbc85576a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0746f1-65e8-4480-9ebb-2601e7b9e8e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eb261e71-9c74-49c2-9121-47703e8b87f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0]\n",
    "\n",
    "\n",
    "\n",
    "def find_connected_indices(subset_highlightIndices1, matrix, highlightIndices1, highlightIndices2):\n",
    "    connected_indices_subset = []\n",
    "    for i, index1 in enumerate(highlightIndices1):\n",
    "        if index1 in subset_highlightIndices1:\n",
    "            # Find the index of the max value in the row\n",
    "            j = matrix.iloc[i].idxmax()\n",
    "            \n",
    "            # Check if the max value is not zero\n",
    "            if matrix.iat[i, j] != 0:\n",
    "                connected_indices_subset.append(highlightIndices2[j])\n",
    "    return np.array(connected_indices_subset)\n",
    "\n",
    "\n",
    "for threshold in thresholds:    \n",
    "    file_path = f'P_octopus_1203_half_half_{threshold}.csv'\n",
    "    matrix_df = pd.read_csv(file_path, header=None)\n",
    "\n",
    "    \n",
    "    # Define the index arrays as provided by the user\n",
    "    highlightIndices1 = idx_1 \n",
    "    highlightIndices2 = idx_2\n",
    "\n",
    "    ## Find non-zero entries in the matrix and map them to the corresponding indices in highlightIndices1 and highlightIndices2\n",
    "    #connected_indices = []\n",
    "    #for i in range(matrix_df.shape[0]):  # Loop over rows\n",
    "       # for j in range(matrix_df.shape[1]):  # Loop over columns\n",
    "          #  if matrix_df.iat[i, j] != 0:  # Check if the matrix element is non-zero\n",
    "           #     connected_indices.append((highlightIndices1[i], highlightIndices2[j]))  # Store the connected indices\n",
    "\n",
    "    # Find the column with the largest value in each row and map them\n",
    "    connected_indices = []\n",
    "    for i in range(matrix_df.shape[0]):  # Loop over rows\n",
    "        # Find the index of the max value in the row\n",
    "        j = matrix_df.iloc[i].idxmax()\n",
    "\n",
    "        # Map and store the indices if the max value is not zero\n",
    "        if matrix_df.iat[i, j] != 0:\n",
    "            connected_indices.append((highlightIndices1[i], highlightIndices2[j]))\n",
    "\n",
    "    ## Find the row with the largest value in each column and map them\n",
    "    #connected_indices = []\n",
    "    #for j in range(matrix_df.shape[1]):  # Loop over columns\n",
    "       # # Find the index of the max value in the column\n",
    "       # i = matrix_df.iloc[:, j].idxmax()\n",
    "\n",
    "       # # Map and store the indices if the max value is not zero\n",
    "       # if matrix_df.iat[i, j] != 0:\n",
    "          #  connected_indices.append((highlightIndices1[i], highlightIndices2[j]))\n",
    "\n",
    "    # Display the connected indices\n",
    "    # connected_indices\n",
    "    \n",
    "    # Convert connected_indices to a DataFrame\n",
    "    connected_indices_df = pd.DataFrame(connected_indices, columns=['Index1', 'Index2'])\n",
    "\n",
    "    first_column = connected_indices_df['Index1']\n",
    "\n",
    "    # Optionally, convert it to a numpy array\n",
    "    sub_highlightIndices1 = first_column.to_numpy()\n",
    "    \n",
    "    second_column = connected_indices_df['Index2']\n",
    "\n",
    "    # Optionally, convert it to a numpy array\n",
    "    sub_highlightIndices2 = second_column.to_numpy()\n",
    "    \n",
    "    # Example usage:\n",
    "    sub_highlightIndices1_leg1 = np.intersect1d(sub_highlightIndices1,indices_less_than_test_1)\n",
    "    # Read from matlab first, remember -1 \n",
    "    \n",
    "    sub_highlightIndices2_leg1 = find_connected_indices(sub_highlightIndices1_leg1, matrix_df, highlightIndices1, highlightIndices2)\n",
    "    \n",
    "    sub_highlightIndices1_leg2 = np.intersect1d(sub_highlightIndices1,indices_less_than_test_2)\n",
    "\n",
    "    # Read from matlab first, remember -1 \n",
    "    sub_highlightIndices2_leg2 = find_connected_indices(sub_highlightIndices1_leg2, matrix_df, highlightIndices1, highlightIndices2)\n",
    "\n",
    "    \n",
    "    sub_highlightIndices1_leg3 = np.intersect1d(sub_highlightIndices1,indices_less_than_test_3)\n",
    "\n",
    "    # Read from matlab first, remember -1 \n",
    "    sub_highlightIndices2_leg3 = find_connected_indices(sub_highlightIndices1_leg3, matrix_df, highlightIndices1, highlightIndices2)\n",
    "\n",
    "    \n",
    "    sub_highlightIndices1_leg4 = np.intersect1d(sub_highlightIndices1,indices_less_than_test_4)\n",
    "\n",
    "    # Read from matlab first, remember -1 \n",
    "    sub_highlightIndices2_leg4 = find_connected_indices(sub_highlightIndices1_leg4, matrix_df, highlightIndices1, highlightIndices2)\n",
    "\n",
    "    \n",
    "    sub_highlightIndices1_leg5 = np.intersect1d(sub_highlightIndices1,indices_less_than_test_5)\n",
    "\n",
    "    # Read from matlab first, remember -1 \n",
    "    sub_highlightIndices2_leg5 = find_connected_indices(sub_highlightIndices1_leg5, matrix_df, highlightIndices1, highlightIndices2)\n",
    "\n",
    "    \n",
    "    sub_highlightIndices1_leg6 = np.intersect1d(sub_highlightIndices1,indices_less_than_test_6)\n",
    "\n",
    "    # Read from matlab first, remember -1 \n",
    "    sub_highlightIndices2_leg6 = find_connected_indices(sub_highlightIndices1_leg6, matrix_df, highlightIndices1, highlightIndices2)\n",
    "\n",
    "    \n",
    "    sub_highlightIndices1_leg7 = np.intersect1d(sub_highlightIndices1,indices_less_than_test_7)\n",
    "\n",
    "    # Read from matlab first, remember -1 \n",
    "    sub_highlightIndices2_leg7 = find_connected_indices(sub_highlightIndices1_leg7, matrix_df, highlightIndices1, highlightIndices2)\n",
    "\n",
    "    \n",
    "    sub_highlightIndices1_leg8 = np.intersect1d(sub_highlightIndices1,indices_less_than_test_8)\n",
    "\n",
    "    # Read from matlab first, remember -1 \n",
    "    sub_highlightIndices2_leg8 = find_connected_indices(sub_highlightIndices1_leg8, matrix_df, highlightIndices1, highlightIndices2)\n",
    "\n",
    "    \n",
    "    sub_highlightIndices1_body = np.intersect1d(sub_highlightIndices1,indices_less_than_test_9)\n",
    "\n",
    "    # Read from matlab first, remember -1 \n",
    "    sub_highlightIndices2_body = find_connected_indices(sub_highlightIndices1_body, matrix_df, highlightIndices1, highlightIndices2)\n",
    "\n",
    "    \n",
    "    # save data\n",
    "    data_to_save = {\n",
    "    \"sub_highlightIndices1\": sub_highlightIndices1.tolist(),\n",
    "    \"sub_highlightIndices2\": sub_highlightIndices2.tolist(), \n",
    "    \"sub_highlightIndices1_body\": sub_highlightIndices1_body.tolist(),\n",
    "    \"sub_highlightIndices2_body\": sub_highlightIndices2_body.tolist(),\n",
    "    \"sub_highlightIndices1_leg1\": sub_highlightIndices1_leg1.tolist(),\n",
    "    \"sub_highlightIndices2_leg1\": sub_highlightIndices2_leg1.tolist(),\n",
    "    \"sub_highlightIndices1_leg2\": sub_highlightIndices1_leg2.tolist(),\n",
    "    \"sub_highlightIndices2_leg2\": sub_highlightIndices2_leg2.tolist(),\n",
    "    \"sub_highlightIndices1_leg3\": sub_highlightIndices1_leg3.tolist(),\n",
    "    \"sub_highlightIndices2_leg3\": sub_highlightIndices2_leg3.tolist(),\n",
    "    \"sub_highlightIndices1_leg4\": sub_highlightIndices1_leg4.tolist(),\n",
    "    \"sub_highlightIndices2_leg4\": sub_highlightIndices2_leg4.tolist(),\n",
    "    \"sub_highlightIndices1_leg5\": sub_highlightIndices1_leg5.tolist(),\n",
    "    \"sub_highlightIndices2_leg5\": sub_highlightIndices2_leg5.tolist(),\n",
    "    \"sub_highlightIndices1_leg6\": sub_highlightIndices1_leg6.tolist(),\n",
    "    \"sub_highlightIndices2_leg6\": sub_highlightIndices2_leg6.tolist(),\n",
    "    \"sub_highlightIndices1_leg7\": sub_highlightIndices1_leg7.tolist(),\n",
    "    \"sub_highlightIndices2_leg7\": sub_highlightIndices2_leg7.tolist(),\n",
    "    \"sub_highlightIndices1_leg8\": sub_highlightIndices1_leg8.tolist(),\n",
    "    \"sub_highlightIndices2_leg8\": sub_highlightIndices2_leg8.tolist(),\n",
    "    }\n",
    "\n",
    "    # Save as JSON file\n",
    "    filename = f'highlight_indices_half_half_{threshold}.json'\n",
    "    with open(filename, 'w') as outfile:\n",
    "        json.dump(data_to_save, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "171f5157-85a5-4278-a89a-c6584dc37f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test sum P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "22081e7a-ab01-46df-ba27-95b04c99bc5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rho: 0.05 Sum of the matrix: 0.08896615170000001\n",
      "rho: 0.1 Sum of the matrix: 0.1543102564\n",
      "rho: 0.2 Sum of the matrix: 0.22006340130000002\n",
      "rho: 0.3 Sum of the matrix: 0.298949745\n",
      "rho: 0.4 Sum of the matrix: 0.3489362639\n",
      "rho: 0.5 Sum of the matrix: 0.4196748134\n",
      "rho: 0.6 Sum of the matrix: 0.396052766\n",
      "rho: 0.7 Sum of the matrix: 0.4240511338\n",
      "rho: 0.8 Sum of the matrix: 0.4039267817\n",
      "rho: 0.9 Sum of the matrix: 0.39324638330000006\n",
      "rho: 1.0 Sum of the matrix: 0.4031787687\n",
      "rho: 1.1 Sum of the matrix: 0.4392453604\n",
      "rho: 1.2 Sum of the matrix: 0.4365533159\n",
      "rho: 1.3 Sum of the matrix: 0.4988240118\n",
      "rho: 1.4 Sum of the matrix: 0.5460681075\n",
      "rho: 1.5 Sum of the matrix: 0.6169342437\n",
      "rho: 1.6 Sum of the matrix: 0.6405562948\n",
      "rho: 1.7 Sum of the matrix: 0.7271704612000001\n",
      "rho: 1.8 Sum of the matrix: 0.8216586472999999\n",
      "rho: 1.9 Sum of the matrix: 0.9161468436000001\n",
      "rho: 2.0 Sum of the matrix: 0.9712649437\n"
     ]
    }
   ],
   "source": [
    "for threshold in thresholds:\n",
    "    filename = f'P_octopus_1203_half_half_{threshold}.csv'\n",
    "    df = pd.read_csv(filename)\n",
    "    matrix = df.values\n",
    "    matrix_sum = np.sum(matrix)\n",
    "    print(\"rho:\", threshold, \"Sum of the matrix:\", matrix_sum)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e6971d-6979-445c-a88d-3981097919f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905957f0-028a-4d94-89ff-e2bbece24bc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
